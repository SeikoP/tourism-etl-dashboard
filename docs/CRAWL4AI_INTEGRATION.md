# Crawl4AI Integration Setup

## ğŸš€ Quick Start

### 1. Start the integrated stack:
```bash
# Start Airflow + Crawl4AI stack
docker-compose up -d

# Check all services
docker-compose ps
```

### 2. Access services:
- **Airflow UI**: http://localhost:8080 (admin/admin)
- **Crawl4AI API**: http://localhost:11235
- **Flower (Celery monitoring)**: http://localhost:5555

### 3. Test Crawl4AI integration:
```bash
# Test Crawl4AI health
curl http://localhost:11235/health

# Run integration test DAG in Airflow UI
# Navigate to: DAGs > crawl4ai_integration_test > Trigger DAG
```

## ğŸ—ï¸ Architecture

```
Tourism ETL Stack:
â”œâ”€â”€ ğŸ”„ Airflow (9 containers)
â”‚   â”œâ”€â”€ Scheduler, Workers, API Server
â”‚   â”œâ”€â”€ Database (PostgreSQL)
â”‚   â””â”€â”€ Message Queue (Redis)
â””â”€â”€ ğŸ¤– Crawl4AI (1 container)
    â”œâ”€â”€ Advanced web scraping
    â”œâ”€â”€ LLM-powered extraction
    â””â”€â”€ API server on port 11235
```

## ğŸ”§ Integration Features

### Traditional Scraping vs Crawl4AI

| Feature | Traditional | Crawl4AI |
|---------|-------------|----------|
| **JavaScript Support** | âŒ Limited | âœ… Full support |
| **Anti-bot Bypass** | âš™ï¸ Manual setup | âœ… Automatic |
| **LLM Extraction** | âŒ None | âœ… GPT-4/Claude support |
| **Parallel Processing** | âœ… AsyncIO | âœ… Built-in |
| **Content Analysis** | âŒ Basic | âœ… Semantic understanding |

### Usage in Airflow DAGs

```python
from services.crawl4ai_integration import VietnamBookingCrawl4AI

def extract_with_crawl4ai():
    crawler = VietnamBookingCrawl4AI()
    hotels = await crawler.extract_hotels_with_crawl4ai(url)
    return hotels
```

## ğŸ“Š Performance Comparison

Run the comparison DAG to see:
- Extraction accuracy
- Speed comparison  
- Data quality metrics
- Success rates

## ğŸ› ï¸ Customization

### Add new extraction strategies:
```python
# CSS-based extraction
result = client.crawl_url(
    url=url,
    extraction_strategy="CSSExtractionStrategy",
    extraction_strategy_args={
        "schema": your_schema
    }
)

# LLM-powered extraction
result = client.extract_with_llm(
    url=url,
    prompt="Extract hotel data as JSON",
    model="gpt-4o-mini"
)
```

### Environment Variables:
```env
# Add to .env file
CRAWL4AI_BASE_DIRECTORY=/app/crawl4ai
CRAWL4AI_LOG_LEVEL=INFO
OPENAI_API_KEY=your_key_here  # For LLM extraction
```

## ğŸ” Monitoring

- **Crawl4AI Logs**: `docker-compose logs crawl4ai`
- **Health Check**: Built-in endpoint `/health`
- **Airflow Integration**: Test DAG for validation
- **Performance Metrics**: Available in Airflow task logs

## âš¡ Benefits

1. **ğŸ¤– Intelligent Extraction**: LLM-powered data parsing
2. **ğŸ›¡ï¸ Better Anti-bot**: Advanced detection evasion
3. **ğŸ“± JS Support**: Full SPA/dynamic content handling
4. **ğŸ”„ Unified Stack**: One docker-compose for everything
5. **ğŸ“Š A/B Testing**: Compare methods in same pipeline